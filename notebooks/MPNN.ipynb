{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af50e956-9d2d-4237-82ee-2d854a1e3f33",
   "metadata": {},
   "source": [
    "# Learning Molecular Representation using Graph Neural Network\n",
    "\n",
    "In this notebook, I will be trying to recreate the molecular property predictor model, [Chemprop](https://github.com/chemprop/chemprop), utilizing graph neural networks. Chemprop adopts a variant of graph neural network called **\"directed message passing neural network (D-MPNN)\"**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b7368-16b1-4cd7-984b-41124606f8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import threading\n",
    "from random import Random\n",
    "from IPython.display import SVG\n",
    "\n",
    "# Import RDKit libraries for chemistry functions\n",
    "import rdkit\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import DataStructs\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "from rdkit.Chem import rdRGroupDecomposition\n",
    "from rdkit.Chem.Draw import IPythonConsole # Needed to display molecules in Jupyter\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import rdDepictor\n",
    "from rdkit.Chem.Draw import rdMolDraw2D\n",
    "from rdkit.Chem.Draw.MolDrawing import MolDrawing, DrawingOptions # Used for custom drawing options\n",
    "\n",
    "# Set drawing options for molecular visualization\n",
    "DrawingOptions.bondLineWidth=1.8\n",
    "IPythonConsole.ipython_useSVG=True  # Use SVG for high-quality molecular graphics\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.warning')  # Disable RDKit warnings for cleaner output\n",
    "print(rdkit.__version__)  # Display the version of RDKit\n",
    "\n",
    "# Import PyTorch libraries for model building\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Import additional utilities\n",
    "from typing import Dict, Iterator, List, Optional, Union, OrderedDict, Tuple\n",
    "from tqdm.notebook import tqdm  # For displaying progress bars\n",
    "from functools import reduce  # For functional programming tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3c6bcd-e4aa-4d42-8edc-20954b3a6a6f",
   "metadata": {},
   "source": [
    "## Defining necessary functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085aef6c-4792-42c4-b427-e65c763c7e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to store training arguments and settings\n",
    "class TrainArgs:\n",
    "    \"\"\"Container for training parameters and model configuration.\"\"\"\n",
    "    smiles_column = None  # Column name for SMILES strings, to be set later\n",
    "    num_workers = 8       # Number of worker threads for loading data\n",
    "    batch_size = 50       # Batch size for training\n",
    "    no_cache_mol = False  # Toggle whether to cache molecule objects\n",
    "    dataset_type = 'regression'  # Type of dataset/task, regression in this case\n",
    "    task_names = []       # Names of the tasks, to be populated based on the dataset\n",
    "    seed = 0              # Random seed for reproducibility\n",
    "    hidden_size = 300     # Hidden layer size for the neural network\n",
    "    bias = False          # Use bias in neural network layers\n",
    "    depth = 3             # Depth of the network (number of layers)\n",
    "    dropout = 0.0         # Dropout rate for training\n",
    "    undirected = False    # Whether the molecular graph is undirected\n",
    "    aggregation = 'mean'  # Method for aggregating node information\n",
    "    aggregation_norm = 100  # Normalization factor for aggregation\n",
    "    ffn_num_layers = 2    # Number of layers in the feed-forward network\n",
    "    ffn_hidden_size = 300 # Hidden layer size of the feed-forward network\n",
    "    init_lr = 1e-4        # Initial learning rate\n",
    "    max_lr = 1e-3         # Maximum learning rate\n",
    "    final_lr = 1e-4       # Final learning rate\n",
    "    num_lrs = 1           # Number of different learning rates to use\n",
    "    warmup_epochs = 2.0   # Number of epochs to linearly increase the learning rate\n",
    "    epochs = 30           # Total number of epochs to train\n",
    "    device = torch.device('cpu')  # Device to run the model on, default to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2990389-6ee2-4b3d-a0ac-2fb70dd236d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and configure the training arguments\n",
    "args = TrainArgs()\n",
    "args.data_path = '../data/enamine_discovery_diversity_set_10240.csv'\n",
    "args.target_column = 'ClogP'  # Column name for the property to predict\n",
    "args.smiles_column = 'SMILES'  # Column name for SMILES strings\n",
    "args.dataset_type = 'regression'  # Type of learning task\n",
    "args.task_names = [args.target_column]  # List of tasks to train on\n",
    "args.num_tasks = 1  # Number of tasks, one in this case since only predicting ClogP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648e6e2-2d87-487b-8a71-28f48dffa989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the caches for graphs and RDKit molecule objects\n",
    "CACHE_GRAPH = True\n",
    "SMILES_TO_GRAPH = {}  # Cache to store pre-computed graph objects from SMILES\n",
    "\n",
    "def cache_graph():\n",
    "    \"\"\"Check if graph caching is enabled.\"\"\"\n",
    "    return CACHE_GRAPH\n",
    "\n",
    "def set_cache_graph(cache_graph: bool):\n",
    "    \"\"\"Enable or disable graph caching.\"\"\"\n",
    "    global CACHE_GRAPH\n",
    "    CACHE_GRAPH = cache_graph\n",
    "\n",
    "CACHE_MOL = True\n",
    "SMILES_TO_MOL: Dict[str, Chem.Mol] = {}  # Cache to store RDKit molecule objects from SMILES\n",
    "\n",
    "def cache_mol() -> bool:\n",
    "    \"\"\"Check if molecule caching is enabled.\"\"\"\n",
    "    return CACHE_MOL\n",
    "\n",
    "def set_cache_mol(cache_mol: bool):\n",
    "    \"\"\"Enable or disable molecule caching.\"\"\"\n",
    "    global CACHE_MOL\n",
    "    CACHE_MOL = cache_mol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc48ca9-b227-4ac3-9121-611e4cfb9d4b",
   "metadata": {},
   "source": [
    "### Defining Atomic and Bond Features\n",
    "The `atom_features` and `bond_features` functions are being used to define the atomic and bond features which will be the node and edge features of the graph neural network. The atom feature vector consists of one-hot encoding of atomic number, degree, formal charge, chirality, number of hydrogens, and hybridization. And the bond feature vector consists of one-hot encoding of bond type (single, double, triple, aromatic) and whether the bond is conjugated or not and whether in the ring or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f438a43-d257-4988-bf41-7987fa0ab279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for atoms and bonds in molecules\n",
    "MAX_ATOMIC_NUM = 100  # Maximum number of atomic numbers to consider for encoding\n",
    "ATOM_FEATURES = {\n",
    "    'atomic_num': list(range(MAX_ATOMIC_NUM)),\n",
    "    'degree': [0, 1, 2, 3, 4, 5],  # Degrees of connectivity for an atom\n",
    "    'formal_charge': [-1, -2, 1, 2, 0],  # Possible formal charges on an atom\n",
    "    'chiral_tag': [0, 1, 2, 3],  # Chirality tags\n",
    "    'num_Hs': [0, 1, 2, 3, 4],  # Number of hydrogen atoms attached\n",
    "    'hybridization': [\n",
    "        Chem.rdchem.HybridizationType.SP,\n",
    "        Chem.rdchem.HybridizationType.SP2,\n",
    "        Chem.rdchem.HybridizationType.SP3,\n",
    "        Chem.rdchem.HybridizationType.SP3D,\n",
    "        Chem.rdchem.HybridizationType.SP3D2\n",
    "    ],  # Hybridization states\n",
    "}\n",
    "\n",
    "PATH_DISTANCE_BINS = list(range(10))  # Binning for path distances in the molecular graph\n",
    "THREE_D_DISTANCE_MAX = 20  # Maximum distance for 3D spatial features\n",
    "THREE_D_DISTANCE_STEP = 1  # Step size for incrementing distance\n",
    "THREE_D_DISTANCE_BINS = list(range(0, THREE_D_DISTANCE_MAX + 1, THREE_D_DISTANCE_STEP))\n",
    "\n",
    "# len(choices) + 1 to include room for uncommon values; + 2 at end for IsAromatic and mass\n",
    "ATOM_FDIM = sum(len(choices) + 1 for choices in ATOM_FEATURES.values()) + 2\n",
    "EXTRA_ATOM_FDIM = 0\n",
    "BOND_FDIM = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31963800-7515-4aa8-8a1f-928ccaaba448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_fdim() -> int:\n",
    "    \"\"\"Calculate the dimensionality of atom features.\"\"\"\n",
    "    return sum(len(choices) + 1 for choices in ATOM_FEATURES.values()) + 2  # Additional features for aromaticity and mass\n",
    "\n",
    "def get_bond_fdim(atom_messages=False) -> int:\n",
    "    \"\"\"Calculate the dimensionality of bond features.\"\"\"\n",
    "    return BOND_FDIM + (not atom_messages) * get_atom_fdim()\n",
    "\n",
    "def onek_encoding_unk(value: int, choices: List[int]):\n",
    "    \"\"\"One-hot encode a value with a provision for unknown values.\"\"\"\n",
    "    encoding = [0] * (len(choices) + 1)\n",
    "    index = choices.index(value) if value in choices else -1\n",
    "    encoding[index] = 1\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74efd73c-54b6-44a4-aad8-4c9ed5291ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atom_features(atom: Chem.rdchem.Atom, functional_groups: List[int] = None):\n",
    "    \"\"\"Build a feature vector for an atom based on its chemical properties.\"\"\"\n",
    "    features = onek_encoding_unk(atom.GetAtomicNum() - 1, ATOM_FEATURES['atomic_num']) + \\\n",
    "               onek_encoding_unk(atom.GetTotalDegree(), ATOM_FEATURES['degree']) + \\\n",
    "               onek_encoding_unk(atom.GetFormalCharge(), ATOM_FEATURES['formal_charge']) + \\\n",
    "               onek_encoding_unk(int(atom.GetChiralTag()), ATOM_FEATURES['chiral_tag']) + \\\n",
    "               onek_encoding_unk(int(atom.GetTotalNumHs()), ATOM_FEATURES['num_Hs']) + \\\n",
    "               onek_encoding_unk(int(atom.GetHybridization()), ATOM_FEATURES['hybridization']) + \\\n",
    "               [1 if atom.GetIsAromatic() else 0] + \\\n",
    "               [atom.GetMass() * 0.01]  # Mass is scaled for feature consistency\n",
    "    if functional_groups is not None:\n",
    "        features += functional_groups\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d669b3d0-3b72-469a-83b3-727ae8503024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bond_features(bond: Chem.rdchem.Bond):\n",
    "    \"\"\"Build a feature vector for a bond based on its chemical properties.\"\"\"\n",
    "    if bond is None:\n",
    "        return [1] + [0] * (BOND_FDIM - 1)  # Handle the case of non-existent bonds\n",
    "    else:\n",
    "        bt = bond.GetBondType()\n",
    "        fbond = [\n",
    "            0,  # Bond is not None\n",
    "            bt == Chem.rdchem.BondType.SINGLE,\n",
    "            bt == Chem.rdchem.BondType.DOUBLE,\n",
    "            bt == Chem.rdchem.BondType.TRIPLE,\n",
    "            bt == Chem.rdchem.BondType.AROMATIC,\n",
    "            bond.GetIsConjugated() if bt is not None else 0,\n",
    "            bond.IsInRing() if bt is not None else 0\n",
    "        ] + onek_encoding_unk(int(bond.GetStereo()), list(range(6)))\n",
    "        return fbond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395ef599-3b5b-4e9f-bbb7-251bafd4733a",
   "metadata": {},
   "source": [
    "#### Visualizing molecular graph with atoms and bonds features\n",
    "Below is an example molecule to see how atom and bond features actually look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e885dc-913b-424b-aa32-bf8c3c5d5eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a moleuclar graph\n",
    "smiles = 'c1ccccc1NC(=O)CC1cncc1'\n",
    "mol = Chem.MolFromSmiles(smiles)\n",
    "mol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233129d4-9212-4f89-a3d6-15bcc4c96726",
   "metadata": {},
   "source": [
    "Below is an interactive visualization of the feature vector of every atoms in the molecule. The first 100 elements represents the atomic number, followed by one hot encodings of degree (2), formal charge (0), chiral (false), total number of Hs (1), hybridization (SP2), aromaticity (1). Finally atomic mass (multiplied by 0.01) at the last entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16aed65-ec4d-48a9-afcd-1d61fb53244c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawit(m, atomId=0):\n",
    "    atom = m.GetAtomWithIdx(atomId)\n",
    "    feat = atom_features(atom)\n",
    "    \n",
    "    # draw molecule with highlight\n",
    "    d = rdMolDraw2D.MolDraw2DSVG(200, 150)\n",
    "    rdMolDraw2D.PrepareAndDrawMolecule(d, m, highlightAtoms=(atom.GetIdx(),))\n",
    "    d.FinishDrawing()\n",
    "    mol_svg = d.GetDrawingText()\n",
    "    \n",
    "    # draw feature\n",
    "    fig = plt.figure(figsize=(3, 0.8), dpi=150)\n",
    "    ax = fig.add_subplot(111)\n",
    "    im = ax.imshow([feat], interpolation='nearest', cmap='viridis', aspect='auto')\n",
    "    plt.xlabel('atom feature')\n",
    "    ax.set_yticks([])\n",
    "    img = BytesIO()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img, transparent=True, format='svg')\n",
    "    plt.close(fig)\n",
    "    feat_svg = img.getvalue().decode()\n",
    "    \n",
    "    # arrange figures\n",
    "    fig1 = sg.fromstring(mol_svg)\n",
    "    fig2 = sg.fromstring(feat_svg)\n",
    "    plot1 = fig1.getroot()\n",
    "    plot2 = fig2.getroot()\n",
    "    plot1.moveto(10, -40)\n",
    "    plot2.moveto(0, 65)\n",
    "\n",
    "    svg = sc.Figure(\"16cm\", \"6cm\", \n",
    "            plot1.scale(0.05), \n",
    "            plot2.scale(0.05), \n",
    "          ).tostr()\n",
    "    return SVG(svg)\n",
    "    \n",
    "interact(drawit, m=fixed(mol), atomId=(0, mol.GetNumAtoms()-1));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200e480a-240d-410f-9fbd-5ec4284f4faa",
   "metadata": {},
   "source": [
    "As seen from the representation, the atom index 0 and 7 are very similar since they are both carbon atoms and only slightly different in terms of aromaticity and the number of hydrogens attached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e62f7-5af0-46b3-ae71-a3e5000376ff",
   "metadata": {},
   "source": [
    "### Graph Featurization\n",
    "The `MolGraph` iterates over atoms and bonds and stores atom feature and bond feature vectors into f_atoms and f_bonds attributes and construct neighboring atom indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b061040d-e28f-4d5f-90b4-19233238d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDatapoint:\n",
    "    \"\"\"\n",
    "    Represents a single molecule's data including features, targets, and its SMILES string.\n",
    "    \"\"\"\n",
    "    def __init__(self, smiles: str, targets: List[Optional[float]] = None, row: OrderedDict = None):\n",
    "        \"\"\"\n",
    "        Initializes a MoleculeDatapoint instance.\n",
    "        Args:\n",
    "            smiles (str): The SMILES string representing the molecule.\n",
    "            targets (List[Optional[float]]): The target properties to predict.\n",
    "            row (OrderedDict): Row from the dataset containing additional data.\n",
    "        \"\"\"\n",
    "        self.smiles = smiles\n",
    "        self.targets = targets\n",
    "        self.features = []\n",
    "        self.row = row\n",
    "\n",
    "    @property\n",
    "    def mol(self) -> Chem.Mol:\n",
    "        \"\"\"\n",
    "        Lazy loads and returns the RDKit molecule object corresponding to the SMILES string.\n",
    "        Caches the molecule if caching is enabled.\n",
    "        \"\"\"\n",
    "        mol = SMILES_TO_MOL.get(self.smiles, Chem.MolFromSmiles(self.smiles))\n",
    "        if cache_mol():\n",
    "            SMILES_TO_MOL[self.smiles] = mol\n",
    "        return mol\n",
    "\n",
    "    def set_features(self, features: np.ndarray) -> None:\n",
    "        \"\"\"Sets the molecule's features.\"\"\"\n",
    "        self.features = features\n",
    "\n",
    "    def extend_features(self, features: np.ndarray) -> None:\n",
    "        \"\"\"Appends additional features to the molecule's feature array.\"\"\"\n",
    "        self.features = np.append(self.features, features) if self.features is not None else features\n",
    "\n",
    "    def num_tasks(self) -> int:\n",
    "        \"\"\"Returns the number of prediction tasks based on the targets list.\"\"\"\n",
    "        return len(self.targets)\n",
    "\n",
    "    def set_targets(self, targets: List[Optional[float]]):\n",
    "        \"\"\"Sets the prediction targets for the molecule.\"\"\"\n",
    "        self.targets = targets\n",
    "\n",
    "    def reset_features_and_targets(self) -> None:\n",
    "        \"\"\"Resets the features and targets to their original values.\"\"\"\n",
    "        self.features, self.targets = self.raw_features, self.raw_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eae036a-b594-4da7-9295-048a5cd29616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch dataset class for handling collections of MoleculeDatapoints.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: List[MoleculeDatapoint]):\n",
    "        \"\"\"\n",
    "        Initializes a MoleculeDataset instance.\n",
    "        Args:\n",
    "            data (List[MoleculeDatapoint]): List of molecule data points.\n",
    "        \"\"\"\n",
    "        self._data = data\n",
    "        self._scaler = None\n",
    "        self._batch_graph = None\n",
    "        self._random = Random()\n",
    "\n",
    "    def smiles(self) -> List[str]:\n",
    "        \"\"\"Returns a list of SMILES strings for all molecules in the dataset.\"\"\"\n",
    "        return [d.smiles for d in self._data]\n",
    "\n",
    "    def mols(self) -> List[Chem.Mol]:\n",
    "        \"\"\"Returns a list of RDKit molecule objects for all molecules in the dataset.\"\"\"\n",
    "        return [d.mol for d in self._data]\n",
    "\n",
    "    def targets(self) -> List[List[Optional[float]]]:\n",
    "        \"\"\"Returns a list of target properties for all molecules in the dataset.\"\"\"\n",
    "        return [d.targets for d in self._data]\n",
    "\n",
    "    def num_tasks(self) -> int:\n",
    "        \"\"\"Returns the number of tasks based on the first data point, assuming uniformity.\"\"\"\n",
    "        return self._data[0].num_tasks() if len(self._data) > 0 else None\n",
    "\n",
    "    def set_targets(self, targets: List[List[Optional[float]]]) -> None:\n",
    "        \"\"\"\n",
    "        Sets the target properties for each molecule in the dataset.\n",
    "        Args:\n",
    "            targets (List[List[Optional[float]]]): Nested list of target values.\n",
    "        \"\"\"\n",
    "        assert len(self._data) == len(targets)\n",
    "        for i in range(len(self._data)):\n",
    "            self._data[i].set_targets(targets[i])\n",
    "\n",
    "    def reset_features_and_targets(self) -> None:\n",
    "        \"\"\"Resets features and targets for all molecules in the dataset to their original values.\"\"\"\n",
    "        for d in self._data:\n",
    "            d.reset_features_and_targets()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of molecules in the dataset.\"\"\"\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, item) -> Union[MoleculeDatapoint, List[MoleculeDatapoint]]:\n",
    "        \"\"\"Returns the molecule data point or a list of data points indexed by item.\"\"\"\n",
    "        return self._data[item]\n",
    "\n",
    "    def batch_graph(self):\n",
    "        \"\"\"\n",
    "        Prepares and caches the graph representation for a batch of molecules, if not already cached.\n",
    "        \"\"\"\n",
    "        if self._batch_graph is None:\n",
    "            self._batch_graph = []\n",
    "            mol_graphs = []\n",
    "            for d in self._data:\n",
    "                mol_graphs_list = []\n",
    "                if d.smiles in SMILES_TO_GRAPH:\n",
    "                    mol_graph = SMILES_TO_GRAPH[d.smiles]\n",
    "                else:\n",
    "                    mol_graph = MolGraph(d.mol)\n",
    "                    if cache_graph():\n",
    "                        SMILES_TO_GRAPH[d.smiles] = mol_graph\n",
    "                mol_graphs.append([mol_graph])\n",
    "            self._batch_graph = [BatchMolGraph([g[i] for g in mol_graphs]) for i in range(len(mol_graphs[0]))]\n",
    "        return self._batch_graph\n",
    "\n",
    "    def features(self) -> List[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns the features associated with each molecule (if they exist).\n",
    "\n",
    "        :return: A list of 1D numpy arrays containing the features for each molecule or None if there are no features.\n",
    "        \"\"\"\n",
    "        if len(self._data) == 0 or self._data[0].features is None:\n",
    "            return None\n",
    "\n",
    "        return [d.features for d in self._data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a0b1a1-4980-41db-8d6a-2db9ea7ffb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_select_ND(source: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Selects the message features from source corresponding to the atom or bond indices in index.\n",
    "    \"\"\"\n",
    "    index_size = index.size()             # (num_atoms/num_bonds, max_num_bonds)\n",
    "    suffix_dim = source.size()[1:]        # (hidden_size,)\n",
    "    final_size = index_size + suffix_dim  # (num_atoms/num_bonds, max_num_bonds, hidden_size)\n",
    "\n",
    "    target = source.index_select(dim=0, index=index.view(-1)) # (num_atoms/num_bonds * max_num_bonds, hidden_size)\n",
    "    target = target.view(final_size)                          # (num_atoms/num_bonds, max_num_bonds, hidden_size)\n",
    "    return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc06f63-4702-4d37-96a7-fa762141afeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolGraph:\n",
    "    \"\"\"\n",
    "    Represents the graph structure of a molecule, including atoms and bonds.\n",
    "    \"\"\"\n",
    "    def __init__(self, mol, atom_descriptors=None):\n",
    "        \"\"\"\n",
    "        Initializes a molecular graph from an RDKit molecule object.\n",
    "        Args:\n",
    "            mol (Chem.Mol or str): RDKit molecule object or SMILES string.\n",
    "            atom_descriptors (Optional): Additional descriptors for atoms, if any.\n",
    "        \"\"\"\n",
    "        # Convert SMILES to RDKit molecule if necessary\n",
    "        if type(mol) == str:\n",
    "            mol = Chem.MolFromSmiles(mol)\n",
    "\n",
    "        self.n_atoms = 0  # number of atoms\n",
    "        self.n_bonds = 0  # number of bonds\n",
    "        self.f_atoms = []  # mapping from atom index to atom features\n",
    "        self.f_bonds = []  # mapping from bond index to concat(in_atom, bond) features\n",
    "        self.a2b = []  # mapping from atom index to incoming bond indices\n",
    "        self.b2a = []  # mapping from bond index to the index of the atom the bond is coming from\n",
    "        self.b2revb = []  # mapping from bond index to the index of the reverse bond\n",
    "\n",
    "        # Get atom features\n",
    "        self.f_atoms = [atom_features(atom) for atom in mol.GetAtoms()]\n",
    "        if atom_descriptors is not None:\n",
    "            self.f_atoms = [f_atoms + descs.tolist() for f_atoms, descs in zip(self.f_atoms, atom_descriptors)]\n",
    "\n",
    "        self.n_atoms = len(self.f_atoms)\n",
    "\n",
    "        # Initialize atom to bond mapping for each atom\n",
    "        for _ in range(self.n_atoms):\n",
    "            self.a2b.append([])\n",
    "\n",
    "        # Get bond features\n",
    "        for a1 in range(self.n_atoms):\n",
    "            for a2 in range(a1 + 1, self.n_atoms):\n",
    "                bond = mol.GetBondBetweenAtoms(a1, a2)\n",
    "\n",
    "                if bond is None:\n",
    "                    continue\n",
    "\n",
    "                f_bond = bond_features(bond)\n",
    "                self.f_bonds.append(self.f_atoms[a1] + f_bond)\n",
    "                self.f_bonds.append(self.f_atoms[a2] + f_bond)\n",
    "\n",
    "                # Update index mappings\n",
    "                b1 = self.n_bonds\n",
    "                b2 = b1 + 1\n",
    "                self.a2b[a2].append(b1)  # b1 = a1 --> a2\n",
    "                self.b2a.append(a1)\n",
    "                self.a2b[a1].append(b2)  # b2 = a2 --> a1\n",
    "                self.b2a.append(a2)\n",
    "                self.b2revb.append(b2)\n",
    "                self.b2revb.append(b1)\n",
    "                self.n_bonds += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e985c9-8554-433a-8dc6-361a481a469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchMolGraph:\n",
    "    \"\"\"\n",
    "    Represents a batch of molecular graphs by aggregating their individual MolGraphs.\n",
    "    \"\"\"\n",
    "    def __init__(self, mol_graphs: List[MolGraph]):\n",
    "        \"\"\"\n",
    "        Initialize a BatchMolGraph object by collating multiple MolGraphs.\n",
    "        Args:\n",
    "            mol_graphs (List[MolGraph]): List of MolGraphs to be batched together.\n",
    "        \"\"\"\n",
    "        self.atom_fdim = get_atom_fdim()\n",
    "        self.bond_fdim = get_bond_fdim()\n",
    "\n",
    "        # Start n_atoms and n_bonds at 1 b/c zero padding\n",
    "        self.n_atoms = 1  # number of atoms (start at 1 b/c need index 0 as padding)\n",
    "        self.n_bonds = 1  # number of bonds (start at 1 b/c need index 0 as padding)\n",
    "        self.a_scope = []  # list of tuples indicating (start_atom_index, num_atoms) for each molecule\n",
    "        self.b_scope = []  # list of tuples indicating (start_bond_index, num_bonds) for each molecule\n",
    "\n",
    "        # All start with zero padding so that indexing with zero padding returns zeros\n",
    "        f_atoms = [[0] * self.atom_fdim]  # atom features\n",
    "        f_bonds = [[0] * self.bond_fdim]  # combined atom/bond features\n",
    "        a2b = [[]]  # mapping from atom index to incoming bond indices\n",
    "        b2a = [0]  # mapping from bond index to the index of the atom the bond is coming from\n",
    "        b2revb = [0]  # mapping from bond index to the index of the reverse bond\n",
    "        for mol_graph in mol_graphs:\n",
    "            f_atoms.extend(mol_graph.f_atoms)\n",
    "            f_bonds.extend(mol_graph.f_bonds)\n",
    "\n",
    "            for a in range(mol_graph.n_atoms):\n",
    "                a2b.append([b + self.n_bonds for b in mol_graph.a2b[a]])\n",
    "\n",
    "            for b in range(mol_graph.n_bonds):\n",
    "                b2a.append(self.n_atoms + mol_graph.b2a[b])\n",
    "                b2revb.append(self.n_bonds + mol_graph.b2revb[b])\n",
    "\n",
    "            self.a_scope.append((self.n_atoms, mol_graph.n_atoms))\n",
    "            self.b_scope.append((self.n_bonds, mol_graph.n_bonds))\n",
    "            self.n_atoms += mol_graph.n_atoms\n",
    "            self.n_bonds += mol_graph.n_bonds\n",
    "\n",
    "        self.max_num_bonds = max(1, max(len(in_bonds) for in_bonds in a2b))  # max with 1 to fix a crash in rare case of all single-heavy-atom mols\n",
    "\n",
    "        self.f_atoms = torch.FloatTensor(f_atoms)\n",
    "        self.f_bonds = torch.FloatTensor(f_bonds)\n",
    "        self.a2b = torch.LongTensor([a2b[a] + [0] * (self.max_num_bonds - len(a2b[a])) for a in range(self.n_atoms)])\n",
    "        self.b2a = torch.LongTensor(b2a)\n",
    "        self.b2revb = torch.LongTensor(b2revb)\n",
    "        self.b2b = None  # try to avoid computing b2b b/c O(n_atoms^3)\n",
    "        self.a2a = None  # only needed if using atom messages\n",
    "\n",
    "    def get_components(self, atom_messages: bool = False) -> Tuple[torch.FloatTensor, torch.FloatTensor,\n",
    "                                                                   torch.LongTensor, torch.LongTensor, torch.LongTensor,\n",
    "                                                                   List[Tuple[int, int]], List[Tuple[int, int]]]:\n",
    "        \"\"\"Provides access to the components of the batch graphs\n",
    "        \"\"\"\n",
    "        return self.f_atoms, self.f_bonds, self.a2b, self.b2a, self.b2revb, self.a_scope, self.b_scope\n",
    "\n",
    "    def get_b2b(self) -> torch.LongTensor:\n",
    "        \"\"\"Computes (if necessary) and returns a mapping from each bond index to all the incoming bond indices.\n",
    "        \"\"\"\n",
    "        if self.b2b is None:\n",
    "            b2b = self.a2b[self.b2a]  # num_bonds x max_num_bonds\n",
    "            # b2b includes reverse edge for each bond so need to mask out\n",
    "            revmask = (b2b != self.b2revb.unsqueeze(1).repeat(1, b2b.size(1))).long()  # num_bonds x max_num_bonds\n",
    "            self.b2b = b2b * revmask\n",
    "\n",
    "        return self.b2b\n",
    "\n",
    "    def get_a2a(self) -> torch.LongTensor:\n",
    "        \"\"\"Computes (if necessary) and returns a mapping from each atom index to all neighboring atom indices.\n",
    "        \"\"\"\n",
    "        if self.a2a is None:\n",
    "            # b = a1 --> a2\n",
    "            # a2b maps a2 to all incoming bonds b\n",
    "            # b2a maps each bond b to the atom it comes from a1\n",
    "            # thus b2a[a2b] maps atom a2 to neighboring atoms a1\n",
    "            self.a2a = self.b2a[self.a2b]  # num_atoms x max_num_bonds\n",
    "\n",
    "        return self.a2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36f376-8025-4bc4-ab72-5945d8c3360e",
   "metadata": {},
   "source": [
    "## Message Passing \n",
    "\n",
    "`Message Passing` is arguably the most interesting part of an MPNN architecture. The messages are passed around according to the connectivity, and the message evolves as it travels around the nodes.\n",
    "\n",
    "The message passing phase consists of $T$ steps of update cycles. In each step, $t$, hidden state $h^t_{vw}$ and message $m^t_{vw}$ are updated using message function $M_t$ and vertex update function $U_t$. Each message and hidden states are associated with nodes $v$ and $w$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e475f894-a22f-4ab5-a24c-e6813f0092f9",
   "metadata": {},
   "source": [
    "The initial hidden state for each node is defined as\n",
    "$$\n",
    "h^0{vw} = T(W_{i}cat(x_v,e_{vw}))\n",
    "$$\n",
    "\n",
    "I will first implement a single message passing cycle and observe the features, then I will see the change in the features after a few more cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8b28a-d8df-4784-909f-cd1e0220c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the tensors for message passing\n",
    "bond_fdim = get_bond_fdim()\n",
    "atom_fdim = get_atom_fdim()\n",
    "n_atoms = 1  # number of atoms (start at 1 b/c need index 0 as padding)\n",
    "n_bonds = 1  # number of bonds (start at 1 b/c need index 0 as padding)\n",
    "\n",
    "a_scope = []  # list of tuples indicating (start_atom_index, num_atoms) for each molecule\n",
    "b_scope = []  # list of tuples indicating (start_bond_index, num_bonds) for each molecule\n",
    "\n",
    "# All start with zero padding so that indexing with zero padding returns zeros\n",
    "f_atoms = [[0] * atom_fdim]  # atom features\n",
    "f_bonds = [[0] * bond_fdim]  # combined atom/bond features\n",
    "a2b = [[]]   # mapping from atom index to incoming bond indices\n",
    "b2a = [0]    # mapping from bond index to the index of the atom the bond is coming from\n",
    "b2revb = [0] # mapping from bond index to the index of the reverse bond\n",
    "\n",
    "f_atoms.extend(mol_graph.f_atoms)\n",
    "f_bonds.extend(mol_graph.f_bonds)\n",
    "\n",
    "for a in range(mol_graph.n_atoms):\n",
    "    a2b.append([b + n_bonds for b in mol_graph.a2b[a]])\n",
    "\n",
    "for b in range(mol_graph.n_bonds):\n",
    "    b2a.append(n_atoms + mol_graph.b2a[b])\n",
    "    b2revb.append(n_bonds + mol_graph.b2revb[b])\n",
    "\n",
    "a_scope.append((n_atoms, mol_graph.n_atoms))\n",
    "b_scope.append((n_bonds, mol_graph.n_bonds))\n",
    "n_atoms += mol_graph.n_atoms\n",
    "n_bonds += mol_graph.n_bonds\n",
    "\n",
    "max_num_bonds = max(1, max(len(in_bonds) for in_bonds in a2b))  # max with 1 to fix a crash in rare case of all single-heavy-atom mols\n",
    "f_atoms = torch.FloatTensor(f_atoms)\n",
    "f_bonds = torch.FloatTensor(f_bonds)\n",
    "a2b = torch.LongTensor([a2b[a] + [0] * (max_num_bonds - len(a2b[a])) for a in range(n_atoms)])\n",
    "b2a = torch.LongTensor(b2a)\n",
    "b2revb = torch.LongTensor(b2revb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3543f7c-569b-46eb-9e28-1586c2ef47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define and initialize leanred matrix\n",
    "input_dim = get_bond_fdim()\n",
    "atom_fdim = get_atom_fdim()\n",
    "\n",
    "W_i = nn.Linear(input_dim, args.hidden_size, bias=args.bias)\n",
    "w_h_input_size = args.hidden_size\n",
    "W_h = nn.Linear(w_h_input_size, args.hidden_size, bias=args.bias)\n",
    "W_o = nn.Linear(atom_fdim + args.hidden_size, args.hidden_size)\n",
    "act_func = nn.ReLU()\n",
    "\n",
    "# initial message\n",
    "input = W_i(torch.FloatTensor(f_bonds))  # num_bonds x hidden_size\n",
    "message = act_func(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a198e-163e-4a43-8983-2817a7256604",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 14))\n",
    "ax = fig.add_subplot(211)\n",
    "im = ax.imshow(mol_graph.f_bonds, interpolation='None', cmap='viridis', aspect='auto')\n",
    "ax.set_yticks(list(range(mol_graph.n_bonds)))\n",
    "ax.set_yticklabels(list(range(mol_graph.n_bonds)))\n",
    "ax.tick_params(left=False)  # remove the ticks\n",
    "plt.xlabel('bond feature')\n",
    "plt.ylabel('bond index')\n",
    "plt.title('Initial bond feature')\n",
    "\n",
    "ax = fig.add_subplot(212)\n",
    "im = ax.imshow(message.detach().numpy(), interpolation='None', cmap='viridis', aspect='auto')\n",
    "ax.set_yticks(list(range(mol_graph.n_bonds + 1)))\n",
    "ax.set_yticklabels(list(range(mol_graph.n_bonds + 1)))\n",
    "ax.tick_params(left=False)  # remove the ticks\n",
    "plt.xlabel('hidden state')\n",
    "plt.ylabel('bond index')\n",
    "plt.title('Initial message')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177e985a-ffe8-4ef9-a1ec-5417fbbd440e",
   "metadata": {},
   "source": [
    "Above is the representation of the features after the initial message, now I will try 3 cycles of message passing and see the change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181cf173-30ee-46e2-a2b5-8196a7aa5e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for depth in range(3):\n",
    "\n",
    "    # m(a1 -> a2) = [sum_{a0 \\in nei(a1)} m(a0 -> a1)] - m(a2 -> a1)\n",
    "    # message      a_message = sum(nei_a_message)      rev_message\n",
    "    nei_a_message = index_select_ND(message, a2b)  # num_atoms x max_num_bonds x hidden\n",
    "    a_message = nei_a_message.sum(dim=1)           # num_atoms x hidden\n",
    "    rev_message = message[b2revb]                  # num_bonds x hidden\n",
    "    message = a_message[b2a] - rev_message         # num_bonds x hidden\n",
    "    message = W_h(message)\n",
    "    message = act_func(input + message)            # num_bonds x hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944961e-3d01-480c-b239-0d605be843c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "im = ax.imshow(a_message[b2a].detach().numpy(), interpolation='None', cmap='viridis', aspect='auto')\n",
    "\n",
    "ax.set_yticks(list(range(mol_graph.n_bonds + 1)))\n",
    "ax.set_yticklabels(list(range(mol_graph.n_bonds + 1)))\n",
    "ax.tick_params(left=False)  # remove the ticks\n",
    "plt.xlabel('hidden state')\n",
    "plt.ylabel('bond index')\n",
    "plt.title('Messages after 3 steps of message passing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f749d-a479-4dfe-871a-469e1e0ad7ff",
   "metadata": {},
   "source": [
    "As seen, the representation of the hidden states have changed significantly during the 3 cycles of message passing.\n",
    "\n",
    "Now that the overall idea of how an MPNN works is clear, I will train a model on a dataset and analyze its accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cb1b15-dd9a-40cf-a098-c25ec445a303",
   "metadata": {},
   "source": [
    "## Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef3f7f0-6d23-4275-bdcc-7c127557ab59",
   "metadata": {},
   "source": [
    "The dataset that I use for this implementation is the Enamine Real's diversity discovery set composed of 10240 compounds. This dataset contains some molecular properties, such as ClogP and TPSA, so we should be able to train a GCNN that predicts those properties. For this example, I'll train the model using ClogP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd479f2e-d907-41ec-b021-8c17b1032201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/enamine_discovery_diversity_set_10240.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77dc5d7-9c41-4c5c-b63b-c86d7c78897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data set\n",
    "data = MoleculeDataset([\n",
    "    MoleculeDatapoint(\n",
    "        smiles=row[args.smiles_column],  # SMILES string from the DataFrame\n",
    "        targets=[row[args.target_column]]  # Target property from the DataFrame\n",
    "    ) for i, row in df.iterrows()  # Iterating over each row in the DataFrame\n",
    "])\n",
    "\n",
    "# Split data into training, validation, and testing sets\n",
    "# Random state for reproducibility and shuffling\n",
    "random = Random()\n",
    "\n",
    "# Define proportions for training, validation, and testing\n",
    "sizes = [0.8, 0.1, 0.1]  # 80% training, 10% validation, 10% test\n",
    "\n",
    "# Generate indices and shuffle them to randomize data points before splitting\n",
    "indices = list(range(len(data)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Calculate split indices\n",
    "train_size = int(sizes[0] * len(data))\n",
    "train_val_size = int((sizes[0] + sizes[1]) * len(data))\n",
    "\n",
    "# Slice the shuffled indices to create training, validation, and testing datasets\n",
    "train = [data[i] for i in indices[:train_size]]\n",
    "val = [data[i] for i in indices[train_size:train_val_size]]\n",
    "test = [data[i] for i in indices[train_val_size:]]\n",
    "\n",
    "# Wrap the lists of data points in MoleculeDataset objects for ease of use with PyTorch DataLoader\n",
    "train_data = MoleculeDataset(train)\n",
    "val_data = MoleculeDataset(val)\n",
    "test_data = MoleculeDataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f9da9-6020-456e-91c9-1107a7eb16d2",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00c29d4-4512-44e9-b057-0f71218a1526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model: nn.Module) -> None:\n",
    "    \"\"\"Initializes the weights of a model in place.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        if param.dim() == 1:\n",
    "            nn.init.constant_(param, 0)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72240178-2157-4226-acca-ffde3a1697ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A PyTorch module for encoding molecular graphs using a Message Passing Neural Network (MPN).\n",
    "    This encoder uses a combination of linear transformations and non-linear activations to process\n",
    "    the graph structure of molecules for downstream prediction tasks.\n",
    "\n",
    "    Attributes:\n",
    "        atom_fdim (int): The feature dimension of each atom.\n",
    "        bond_fdim (int): The feature dimension of each bond.\n",
    "        hidden_size (int): The size of the hidden layers.\n",
    "        bias (bool): Whether to add a bias term in the linear transformations.\n",
    "        depth (int): The number of message passing iterations.\n",
    "        dropout (float): Dropout rate for regularization during training.\n",
    "        layers_per_message (int): Number of layers per message passing iteration (currently set statically to 1).\n",
    "        undirected (bool): Whether the molecular graph is undirected (currently not used).\n",
    "        atom_messages (bool): Whether messages are being passed at the atom level (currently not used).\n",
    "        device (torch.device): The device (CPU or GPU) on which computations will be performed.\n",
    "        aggregation (str): The method of aggregating messages ('mean', 'sum', 'norm').\n",
    "        aggregation_norm (int): Normalization constant used when aggregation is 'norm'.\n",
    "\n",
    "    Methods:\n",
    "        forward(mol_graph): Processes a batch of molecular graphs and returns their vector representations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, atom_fdim, bond_fdim):\n",
    "        super(MPNEncoder, self).__init__()\n",
    "        self.atom_fdim = atom_fdim\n",
    "        self.bond_fdim = bond_fdim\n",
    "        self.hidden_size = args.hidden_size\n",
    "        self.bias = args.bias\n",
    "        self.depth = args.depth\n",
    "        self.dropout = args.dropout\n",
    "        self.layers_per_message = 1\n",
    "        self.undirected = False\n",
    "        self.atom_messages = False\n",
    "        self.device = args.device\n",
    "        self.aggregation = args.aggregation\n",
    "        self.aggregation_norm = args.aggregation_norm\n",
    "\n",
    "        # Layers\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "        self.act_func = nn.ReLU()\n",
    "        self.cached_zero_vector = nn.Parameter(torch.zeros(self.hidden_size), requires_grad=False)\n",
    "\n",
    "        # Weight matrices for input and hidden layers\n",
    "        self.W_i = nn.Linear(self.bond_fdim, self.hidden_size, bias=self.bias)\n",
    "        self.W_h = nn.Linear(self.hidden_size, self.hidden_size, bias=self.bias)\n",
    "        self.W_o = nn.Linear(self.atom_fdim + self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, mol_graph):\n",
    "        \"\"\"\n",
    "        Forward pass of the MPNEncoder.\n",
    "\n",
    "        Args:\n",
    "            mol_graph (BatchMolGraph): A batch of molecular graphs to encode.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of molecular vectors, each representing a molecule in the batch.\n",
    "        \"\"\"\n",
    "        # Extract graph components\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb, a_scope, b_scope = mol_graph.get_components()\n",
    "        f_atoms, f_bonds, a2b, b2a, b2revb = f_atoms.to(self.device), f_bonds.to(self.device), a2b.to(self.device), b2a.to(self.device), b2revb.to(self.device)\n",
    "\n",
    "        # Initial message passing\n",
    "        input = self.W_i(f_bonds)\n",
    "        message = self.act_func(input)\n",
    "\n",
    "        # Iterative message passing\n",
    "        for depth in range(self.depth - 1):\n",
    "            nei_a_message = index_select_ND(message, a2b)\n",
    "            a_message = nei_a_message.sum(dim=1)\n",
    "            rev_message = message[b2revb]\n",
    "            message = self.W_h(a_message[b2a] - rev_message)\n",
    "            message = self.act_func(input + message)\n",
    "            message = self.dropout_layer(message)\n",
    "\n",
    "        # Aggregating messages to atom vectors\n",
    "        nei_a_message = index_select_ND(message, a2b)\n",
    "        a_message = nei_a_message.sum(dim=1)\n",
    "        a_input = torch.cat([f_atoms, a_message], dim=1)\n",
    "        atom_hiddens = self.act_func(self.W_o(a_input))\n",
    "        atom_hiddens = self.dropout_layer(atom_hiddens)\n",
    "\n",
    "        # Readout phase to combine atom vectors into molecule vectors\n",
    "        mol_vecs = []\n",
    "        for i, (a_start, a_size) in enumerate(a_scope):\n",
    "            if a_size == 0:\n",
    "                mol_vecs.append(self.cached_zero_vector)\n",
    "            else:\n",
    "                cur_hiddens = atom_hiddens.narrow(0, a_start, a_size)\n",
    "                mol_vec = cur_hiddens\n",
    "                if self.aggregation == 'mean':\n",
    "                    mol_vec = mol_vec.sum(dim=0) / a_size\n",
    "                elif self.aggregation == 'sum':\n",
    "                    mol_vec = mol_vec.sum(dim=0)\n",
    "                elif self.aggregation == 'norm':\n",
    "                    mol_vec = mol_vec.sum(dim=0) / self.aggregation_norm\n",
    "                mol_vecs.append(mol_vec)\n",
    "\n",
    "        mol_vecs = torch.stack(mol_vecs, dim=0)\n",
    "\n",
    "        return mol_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8eb383-6ad1-4f13-9bc5-f993e53fd6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPN(nn.Module):\n",
    "    \"\"\"\n",
    "    Message Passing Network (MPN) for encoding molecular graphs into a vector representation.\n",
    "\n",
    "    This class wraps the MPNEncoder and provides an interface to process batches of molecular graphs, converting them into fixed-size embeddings that can be used for downstream tasks like property prediction.\n",
    "\n",
    "    Attributes:\n",
    "        atom_fdim (int): The feature dimension for atoms, automatically determined if not provided.\n",
    "        bond_fdim (int): The feature dimension for bonds, automatically determined if not provided.\n",
    "        device (torch.device): The device on which to perform calculations (CPU or GPU).\n",
    "        encoder (MPNEncoder): The encoder that performs the message passing and encoding logic.\n",
    "\n",
    "    Methods:\n",
    "        forward(batch): Processes a batch of molecules through the encoder and aggregates the results.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, atom_fdim=None, bond_fdim=None):\n",
    "        \"\"\"\n",
    "        Initializes the MPN model with the necessary settings and sub-models.\n",
    "        \n",
    "        Args:\n",
    "            args: Configuration arguments which contain settings like the device and dimensions.\n",
    "            atom_fdim (int, optional): The feature dimension for atoms. If None, uses a default function.\n",
    "            bond_fdim (int, optional): The feature dimension for bonds. If None, uses a default function.\n",
    "        \"\"\"\n",
    "        super(MPN, self).__init__()\n",
    "        self.atom_fdim = atom_fdim or get_atom_fdim()  # Set atom feature dimension\n",
    "        self.bond_fdim = bond_fdim or get_bond_fdim()  # Set bond feature dimension\n",
    "        self.device = args.device  # Device configuration\n",
    "        self.encoder = MPNEncoder(args, self.atom_fdim, self.bond_fdim)  # Initialize the encoder\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Forward pass through the network which processes a batch of molecular data.\n",
    "\n",
    "        Args:\n",
    "            batch (list): A batch of data that needs to be processed. Each element can be a molecule graph or data structure that the encoder can process.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor containing the encoded representations of the batch.\n",
    "        \"\"\"\n",
    "        # Ensure that each item in the batch is a MolGraph; if not, convert it\n",
    "        if type(batch[0]) != BatchMolGraph:\n",
    "            batch = [mol2graph(b) for b in batch]  # Convert data to MolGraph if necessary\n",
    "\n",
    "        # Encode the batch using the MPNEncoder\n",
    "        encodings = [self.encoder(batch[0])]\n",
    "        # Combine encodings if there are multiple (this example assumes a single encoding for simplicity)\n",
    "        output = reduce(lambda x, y: torch.cat((x, y), dim=1), encodings)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0adb51-8b4c-4917-bc32-0dcdd6bca6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolGraphNet(nn.Module):\n",
    "    \"\"\"\n",
    "    The primary model for molecular property prediction, integrating the MPN encoder and a fully connected feed-forward network (FFN) for final prediction.\n",
    "\n",
    "    Attributes:\n",
    "        classification (bool): Indicates if the model is used for classification tasks.\n",
    "        featurizer (bool): Flag to determine if the model should act as a featurizer, i.e., outputting features rather than predictions.\n",
    "        output_size (int): The number of output targets or classes.\n",
    "        sigmoid (nn.Module, optional): Sigmoid activation, used in the output layer for classification tasks.\n",
    "        encoder (MPN): An instance of the MPN class, which encodes molecular graphs.\n",
    "        ffn (nn.Sequential): The feed-forward network that processes encoded molecular features into final predictions.\n",
    "\n",
    "    Methods:\n",
    "        create_encoder(args): Initializes the MPN encoder.\n",
    "        create_ffn(args): Constructs the feed-forward layers based on the provided arguments.\n",
    "        featurize(batch, features_batch=None, atom_descriptors_batch=None): Generates features from the input batch using the model.\n",
    "        forward(batch): Processes input through the encoder and FFN to produce predictions or features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args, featurizer=False):\n",
    "        super(MoleculeModel, self).__init__()\n",
    "        self.classification = args.dataset_type == 'classification'\n",
    "        self.featurizer = featurizer\n",
    "        self.output_size = args.num_tasks\n",
    "\n",
    "        if self.classification:\n",
    "            self.sigmoid = nn.Sigmoid()  # Activation function for binary classification output\n",
    "\n",
    "        # Initialize the components of the model\n",
    "        self.create_encoder(args)\n",
    "        self.create_ffn(args)\n",
    "        initialize_weights(self)  # Custom function to initialize model weights\n",
    "\n",
    "    def create_encoder(self, args):\n",
    "        \"\"\"\n",
    "        Initializes the encoder component of the model using the MPN architecture.\n",
    "        \"\"\"\n",
    "        self.encoder = MPN(args)\n",
    "\n",
    "    def create_ffn(self, args):\n",
    "        \"\"\"\n",
    "        Constructs the feed-forward network using the specifications provided in args.\n",
    "\n",
    "        Args:\n",
    "            args: Configuration arguments containing network settings.\n",
    "        \"\"\"\n",
    "        first_linear_dim = args.hidden_size\n",
    "        dropout = nn.Dropout(args.dropout)\n",
    "        activation = nn.ReLU()\n",
    "\n",
    "        # Build the FFN architecture dynamically based on the number of layers specified\n",
    "        if args.ffn_num_layers == 1:\n",
    "            ffn = [dropout, nn.Linear(first_linear_dim, self.output_size)]\n",
    "        else:\n",
    "            ffn = [dropout, nn.Linear(first_linear_dim, args.ffn_hidden_size)]\n",
    "            for _ in range(args.ffn_num_layers - 2):\n",
    "                ffn.extend([activation, dropout, nn.Linear(args.ffn_hidden_size, args.ffn_hidden_size)])\n",
    "            ffn.extend([activation, dropout, nn.Linear(args.ffn_hidden_size, self.output_size)])\n",
    "\n",
    "        self.ffn = nn.Sequential(*ffn)  # Wrap the list of layers into a Sequential container\n",
    "\n",
    "    def featurize(self, batch, features_batch=None, atom_descriptors_batch=None):\n",
    "        \"\"\"\n",
    "        Generates feature vectors from the input batch, omitting the final prediction layer.\n",
    "\n",
    "        Args:\n",
    "            batch: The batch of data to process.\n",
    "            features_batch: Optional additional features to be included.\n",
    "            atom_descriptors_batch: Optional atomic descriptors to be included.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The feature vectors for the batch.\n",
    "        \"\"\"\n",
    "        return self.ffn[:-1](self.encoder(batch))  # Exclude the last layer (prediction layer)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\"\n",
    "        Defines the forward pass through the model.\n",
    "\n",
    "        Args:\n",
    "            batch: The batch of molecular graphs to be processed.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output predictions or class probabilities.\n",
    "        \"\"\"\n",
    "        output = self.ffn(self.encoder(batch))\n",
    "        if self.classification and not self.training:\n",
    "            output = self.sigmoid(output)  # Apply sigmoid for classification to get probabilities\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cc2da1-7cbc-4ebb-9523-37969b1d13ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MolGraphNet(args)\n",
    "model = model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90daf01d-9c46-4f9d-9efd-5276dc27027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b65d3-0249-4796-982a-1860906e4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamLR(_LRScheduler):\n",
    "    \"\"\"\n",
    "    Implements a learning rate scheduler that adjusts the learning rate according to the Noam scheme.\n",
    "    It starts with a linear warm-up phase, followed by an exponential decay.\n",
    "\n",
    "    Attributes:\n",
    "        optimizer (Optimizer): The optimizer for which to adjust the learning rate.\n",
    "        warmup_epochs (list of floats or ints): Number of epochs during which the learning rate increases.\n",
    "        total_epochs (list of ints): Total number of epochs for training.\n",
    "        steps_per_epoch (int): Number of batches (steps) per epoch.\n",
    "        init_lr (list of floats): Initial learning rates for each parameter group.\n",
    "        max_lr (list of floats): Maximum learning rates during the warm-up.\n",
    "        final_lr (list of floats): Final learning rates after decay.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_epochs, total_epochs, steps_per_epoch, init_lr, max_lr, final_lr):\n",
    "        \"\"\"\n",
    "        Initializes the NoamLR scheduler.\n",
    "\n",
    "        Args:\n",
    "            optimizer (Optimizer): Bound optimizer.\n",
    "            warmup_epochs (list of float|int): Epochs to linearly increase the learning rate.\n",
    "            total_epochs (list of int): Total duration of training to adjust learning rate.\n",
    "            steps_per_epoch (int): Number of optimizer updates per epoch.\n",
    "            init_lr (list of float): Initial learning rates for each parameter group.\n",
    "            max_lr (list of float): Peak learning rates to reach after warmup.\n",
    "            final_lr (list of float): Learning rates to decay towards by the end of training.\n",
    "        \"\"\"\n",
    "        assert len(optimizer.param_groups) == len(warmup_epochs) == len(total_epochs) == \\\n",
    "               len(init_lr) == len(max_lr) == len(final_lr), \"Length of constructor arguments must match.\"\n",
    "\n",
    "        self.num_lrs = len(optimizer.param_groups)\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_epochs = np.array(warmup_epochs)\n",
    "        self.total_epochs = np.array(total_epochs)\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.init_lr = np.array(init_lr)\n",
    "        self.max_lr = np.array(max_lr)\n",
    "        self.final_lr = np.array(final_lr)\n",
    "\n",
    "        self.current_step = 0\n",
    "        self.lr = init_lr\n",
    "        self.warmup_steps = (self.warmup_epochs * self.steps_per_epoch).astype(int)\n",
    "        self.total_steps = self.total_epochs * self.steps_per_epoch\n",
    "        self.linear_increment = (self.max_lr - self.init_lr) / self.warmup_steps\n",
    "        self.exponential_gamma = (self.final_lr / self.max_lr) ** (1 / (self.total_steps - self.warmup_steps))\n",
    "\n",
    "        super(NoamLR, self).__init__(optimizer)\n",
    "\n",
    "    def get_lr(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the current learning rate for each parameter group.\n",
    "        \"\"\"\n",
    "        return list(self.lr)\n",
    "\n",
    "    def step(self, current_step=None):\n",
    "        \"\"\"\n",
    "        Update the learning rate after each batch iteration.\n",
    "\n",
    "        Args:\n",
    "            current_step (int, optional): Optionally specify the current training step. If not provided,\n",
    "            the internal step counter is used and incremented.\n",
    "        \"\"\"\n",
    "        if current_step is not None:\n",
    "            self.current_step = current_step\n",
    "        else:\n",
    "            self.current_step += 1\n",
    "\n",
    "        for i in range(self.num_lrs):\n",
    "            if self.current_step <= self.warmup_steps[i]:\n",
    "                self.lr[i] = self.init_lr[i] + self.current_step * self.linear_increment[i]\n",
    "            elif self.current_step <= self.total_steps[i]:\n",
    "                self.lr[i] = self.max_lr[i] * (self.exponential_gamma[i] ** (self.current_step - self.warmup_steps[i]))\n",
    "            else:\n",
    "                self.lr[i] = self.final_lr[i]\n",
    "\n",
    "            self.optimizer.param_groups[i]['lr'] = self.lr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b576de4-d2cd-4f0b-aa05-32ab8ede4924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_molecule_batch(data):\n",
    "    \"\"\"\n",
    "    Prepare and process a batch of molecular data for use in a DataLoader.\n",
    "\n",
    "    This function takes a list of data points, wraps them into a MoleculeDataset object,\n",
    "    and ensures that all necessary graph computations for the batch are performed in advance.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of molecular data points.\n",
    "\n",
    "    Returns:\n",
    "        MoleculeDataset: A dataset object with precomputed graph representations.\n",
    "    \"\"\"\n",
    "    data = MoleculeDataset(data)\n",
    "    data.batch_graph()  # Ensures all molecular graphs are processed and ready for model input\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45602202-b4b5-4856-b843-948b1e6d684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Custom sampler for molecular datasets, supporting optional shuffling.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (Dataset): The dataset from which to sample.\n",
    "        shuffle (bool): Whether to shuffle the data every epoch.\n",
    "        _random (Random): Random number generator for shuffling.\n",
    "        length (int): Number of items in the dataset.\n",
    "\n",
    "    Methods:\n",
    "        __iter__: Returns an iterator over the dataset indices.\n",
    "        __len__: Returns the number of items in the dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, shuffle=False, seed=0):\n",
    "        super(Sampler, self).__init__()\n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self._random = Random(seed)  # Random generator for shuffling\n",
    "        self.length = len(self.dataset)  # Store dataset length for easy access\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        if self.shuffle:\n",
    "            self._random.shuffle(indices)  # Shuffle indices if required\n",
    "        return iter(indices)  # Return iterator over indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length  # Return the length of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc11a8-ff44-43b1-bb5b-095e24fef44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoleculeDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "    DataLoader for handling molecular data with specific needs like batch processing and optional shuffling.\n",
    "\n",
    "    Extends PyTorch DataLoader, configuring it specifically for molecular datasets using a custom sampler\n",
    "    and data preparation function.\n",
    "\n",
    "    Attributes:\n",
    "        _dataset (MoleculeDataset): The dataset to load.\n",
    "        _batch_size (int): Number of items per batch.\n",
    "        _num_workers (int): Number of subprocesses to use for data loading.\n",
    "        _shuffle (bool): Whether to shuffle the data at every epoch.\n",
    "        _seed (int): Random seed for shuffling.\n",
    "        _context (str, optional): Multiprocessing context, set to 'forkserver' if not the main thread to avoid hangs.\n",
    "        _timeout (int): Timeout for collecting a batch from workers.\n",
    "        _sampler (Sampler): Custom sampler to manage the sampling of data indices based on shuffling and seeding.\n",
    "\n",
    "    Methods:\n",
    "        targets: Property that attempts to return targets from the dataset unless shuffling or class balance is active.\n",
    "        iter_size: Property returning the size of an iterator over the dataset.\n",
    "        __iter__: Provides an iterator over batches of data.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size=50, num_workers=8, shuffle=False, seed=0):\n",
    "        self._dataset = dataset\n",
    "        self._batch_size = batch_size\n",
    "        self._num_workers = num_workers\n",
    "        self._shuffle = shuffle\n",
    "        self._seed = seed\n",
    "        is_main_thread = threading.current_thread() is threading.main_thread()\n",
    "\n",
    "        if not is_main_thread and self._num_workers > 0:\n",
    "            self._context = 'forkserver'  # Use forkserver to avoid issues in non-main threads\n",
    "            self._timeout = 3600  # Set a long timeout to handle possible delays in data loading\n",
    "\n",
    "        self._sampler = MoleculeSampler(dataset=self._dataset, shuffle=self._shuffle, seed=self._seed)\n",
    "\n",
    "        super(MoleculeDataLoader, self).__init__(\n",
    "            dataset=self._dataset,\n",
    "            batch_size=self._batch_size,\n",
    "            sampler=self._sampler,\n",
    "            num_workers=self._num_workers,\n",
    "            collate_fn=construct_molecule_batch,\n",
    "            multiprocessing_context=self._context,\n",
    "            timeout=self._timeout\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def targets(self):\n",
    "        \"\"\"\n",
    "        Retrieves targets from the dataset unless shuffle is enabled, in which case raising an error.\n",
    "        \n",
    "        Returns:\n",
    "            List[List[Optional[float]]]: List of target values from the dataset.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If data shuffling is enabled, as target retrieval might not be safe.\n",
    "        \"\"\"\n",
    "        if self._class_balance or self._shuffle:\n",
    "            raise ValueError('Cannot safely extract targets when class balance or shuffle are enabled.')\n",
    "        return [self._dataset[index].targets for index in self._sampler]\n",
    "\n",
    "    @property\n",
    "    def iter_size(self):\n",
    "        \"\"\"Returns the number of items that can be iterated over.\"\"\"\n",
    "        return len(self._sampler)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"Provides an iterator over the batches of data.\"\"\"\n",
    "        return super(MoleculeDataLoader, self).__iter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50823892-c557-4f13-b82c-5a4cc28b0289",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4756cb99-a0f6-4330-89da-dab02f4b1ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the training data loader\n",
    "train_data_loader = MoleculeDataLoader(\n",
    "    dataset=train_data,  # The training dataset prepared earlier\n",
    "    batch_size=args.batch_size,  # Number of samples per batch as specified in arguments\n",
    "    num_workers=0,  # Number of worker processes to use for data loading\n",
    "    shuffle=True,  # Enable shuffling to randomize the order of data for each epoch\n",
    "    seed=args.seed  # Seed for the random number generator used in shuffling\n",
    ")\n",
    "\n",
    "# Initialize the validation data loader\n",
    "val_data_loader = MoleculeDataLoader(\n",
    "    dataset=val_data,  # The validation dataset prepared earlier\n",
    "    batch_size=args.batch_size,  # Number of samples per batch, consistent with the training loader\n",
    "    num_workers=0,  # Validation typically requires less parallelism, hence set to 0\n",
    "    shuffle=False  # Shuffling not necessary for validation runs\n",
    ")\n",
    "\n",
    "# Initialize the testing data loader\n",
    "test_data_loader = MoleculeDataLoader(\n",
    "    dataset=test_data,  # The testing dataset prepared earlier\n",
    "    batch_size=args.batch_size,  # Consistent batch size across all loaders\n",
    "    num_workers=0,  # Similar to validation, parallelism is less of a concern during testing\n",
    "    shuffle=False  # Shuffling is generally not applied during testing to maintain predictable order\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cb43f-bf91-4f93-8d45-ef96990424d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "params = [{'params': model.parameters(), 'lr': args.init_lr, 'weight_decay': 0}]\n",
    "optimizer = Adam(params)\n",
    "\n",
    "# scheduler\n",
    "scheduler = NoamLR(\n",
    "    optimizer=optimizer,\n",
    "    warmup_epochs=[args.warmup_epochs],\n",
    "    total_epochs=[args.epochs] * args.num_lrs,\n",
    "    steps_per_epoch=len(train_data) // args.batch_size,\n",
    "    init_lr=[args.init_lr],\n",
    "    max_lr=[args.max_lr],\n",
    "    final_lr=[args.final_lr]\n",
    ")\n",
    "\n",
    "# loss function\n",
    "loss_func = nn.MSELoss(reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cbe959-3d79-4df3-95c9-38b014612da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train loop\n",
    "model.train()\n",
    "loss_sum = iter_count = 0\n",
    "n_iter = 0\n",
    "\n",
    "for batch in tqdm(train_data_loader, total=len(train_data_loader), leave=False):\n",
    "    mol_batch, target_batch = batch.batch_graph(), batch.targets()\n",
    "    mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])\n",
    "    targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])\n",
    "\n",
    "    # Run model\n",
    "    model.zero_grad()\n",
    "    preds = model(mol_batch)\n",
    "\n",
    "    # Move tensors to correct device\n",
    "    mask = mask.to(preds.device)\n",
    "    targets = targets.to(preds.device)\n",
    "    class_weights = torch.ones(targets.shape, device=preds.device)\n",
    "\n",
    "    loss = loss_func(preds, targets) * class_weights * mask\n",
    "    loss = loss.sum() / mask.sum()\n",
    "\n",
    "    loss_sum += loss.item()\n",
    "    iter_count += 1\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if isinstance(scheduler, NoamLR):\n",
    "        scheduler.step()\n",
    "\n",
    "    n_iter += len(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6de06e-ea4f-43f1-b493-015da1e1f2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "initial_preds = []\n",
    "\n",
    "for batch in tqdm(val_data_loader, disable=False, leave=False):\n",
    "    # Prepare batch\n",
    "    batch: MoleculeDataset\n",
    "    mol_batch = batch.batch_graph()\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        batch_preds = model(mol_batch)\n",
    "\n",
    "    batch_preds = batch_preds.data.cpu().numpy()\n",
    "\n",
    "    # Collect vectors\n",
    "    batch_preds = batch_preds.tolist()\n",
    "    initial_preds.extend(batch_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e82fd5-c501-4e0e-98e7-51d3edfc6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_preds and valid_targets have shape (num_tasks, data_size)\n",
    "targets = val_data_loader.targets\n",
    "metric_func = mean_squared_error\n",
    "\n",
    "valid_preds = [[] for _ in range(args.num_tasks)]\n",
    "valid_targets = [[] for _ in range(args.num_tasks)]\n",
    "for i in range(args.num_tasks):\n",
    "    for j in range(len(preds)):\n",
    "        if targets[j][i] is not None:  # Skip those without targets\n",
    "            valid_preds[i].append(preds[j][i].detach())\n",
    "            valid_targets[i].append(targets[j][i])\n",
    "            \n",
    "result = metric_func(valid_targets[i], valid_preds[i])\n",
    "print('MSE:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19b3d5-6f65-4e66-8fcb-46dcb527ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = plt.figure(figsize=(4,4))\n",
    "plt.scatter(valid_targets[i], valid_preds[i])\n",
    "plt.xlabel('Target ClogP')\n",
    "plt.ylabel('Predicted ClogP')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feafc02d-4269-462e-a656-aa6b44c0b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = NoamLR(\n",
    "    optimizer=optimizer,\n",
    "    warmup_epochs=[args.warmup_epochs],\n",
    "    total_epochs=[args.epochs] * args.num_lrs,\n",
    "    steps_per_epoch=len(train_data) // args.batch_size,\n",
    "    init_lr=[args.init_lr],\n",
    "    max_lr=[args.max_lr],\n",
    "    final_lr=[args.final_lr]\n",
    ")\n",
    "\n",
    "loss_func = nn.MSELoss(reduction='none')\n",
    "\n",
    "optimizer = Adam(params)\n",
    "\n",
    "metric_func = mean_squared_error\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "\n",
    "    # train \n",
    "    model.train()\n",
    "\n",
    "    loss_sum = iter_count = 0\n",
    "    n_iter = 0\n",
    "\n",
    "    for batch in tqdm(train_data_loader, total=len(train_data_loader), leave=False):\n",
    "        mol_batch, target_batch = batch.batch_graph(), batch.targets()\n",
    "        mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])\n",
    "        targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])\n",
    "\n",
    "        # Run model\n",
    "        model.zero_grad()\n",
    "        preds = model(mol_batch)\n",
    "\n",
    "        # Move tensors to correct device\n",
    "        mask = mask.to(preds.device)\n",
    "        targets = targets.to(preds.device)\n",
    "        class_weights = torch.ones(targets.shape, device=preds.device)\n",
    "\n",
    "        loss = loss_func(preds, targets) * class_weights * mask\n",
    "        loss = loss.sum() / mask.sum()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "        iter_count += 1\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if isinstance(scheduler, NoamLR):\n",
    "            scheduler.step()\n",
    "\n",
    "        n_iter += len(batch)\n",
    "\n",
    "    # eval\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for batch in tqdm(val_data_loader, disable=False, leave=False):\n",
    "        # Prepare batch\n",
    "        batch: MoleculeDataset\n",
    "        mol_batch = batch.batch_graph()\n",
    "\n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            batch_preds = model(mol_batch)\n",
    "\n",
    "        batch_preds = batch_preds.data.cpu().numpy()\n",
    "\n",
    "        # Collect vectors\n",
    "        batch_preds = batch_preds.tolist()\n",
    "        preds.extend(batch_preds)\n",
    "\n",
    "    # valid_preds and valid_targets have shape (num_tasks, data_size)\n",
    "    num_tasks = 1\n",
    "    targets = val_data_loader.targets\n",
    "\n",
    "    valid_preds = [[] for _ in range(num_tasks)]\n",
    "    valid_targets = [[] for _ in range(num_tasks)]\n",
    "    for i in range(num_tasks):\n",
    "        for j in range(len(preds)):\n",
    "            if targets[j][i] is not None:  # Skip those without targets\n",
    "                valid_preds[i].append(preds[j][i])\n",
    "                valid_targets[i].append(targets[j][i])\n",
    "\n",
    "    result = metric_func(valid_targets[i], valid_preds[i])\n",
    "    print(epoch, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77584807-7bca-4d12-9fcf-49d596688c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = plt.figure(figsize=(4,4))\n",
    "plt.scatter(valid_targets[i], valid_preds[i])\n",
    "plt.xlabel('Target ClogP')\n",
    "plt.ylabel('Predicted ClogP')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
